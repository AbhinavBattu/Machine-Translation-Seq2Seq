# Machine-Translation-Seq2Seq

Built a Sequence-to-Sequence (Seq2Seq) model for machine translation using PyTorch. Implemented an encoder-decoder architecture with LSTM layers to translate sentences from one language to another. The model captures long-term dependencies in text, making it effective for handling varying sentence structures across languages.

# Blog Link

For a deeper dive into the underlying Transformer architecture, check out my blog post: [Understanding How a Seq2Seq Model Works for Machine Translation](https://medium.com/@abhinavbattu88/understanding-how-a-seq2seq-model-works-for-machine-translation-comprehensive-explanation-for-each-d1d872d67e9a)
