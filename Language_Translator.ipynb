{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13de604-5403-4e40-a40e-1f86ba7c7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets evaluate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c204847-6987-462e-b3a0-d4c243cbe379",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1c66c-2426-4dde-879f-9a42b06ddd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2103f1-71d7-451a-80a6-cfc1abe1f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c360def-b17d-4576-8c0a-57fbcd3e00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccc776-64ac-4519-8e72-0024a79afffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=datasets.load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f4272-0b24-4393-b8cf-696d0c8e1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5faa3a-b430-440d-ae67-1838a6e192b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data=(dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581ebed-0b7d-4d53-bb8a-a754898c8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f58cfa-8822-4ced-868d-695b6a58a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d5620-ab6a-4275-bd47-9b40c6167247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd249684-6924-46be-9ec7-e0f63f1ef294",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp=spacy.load(\"en_core_web_sm\")\n",
    "de_nlp=spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c6656-b22d-4668-a984-304d174e3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text,en_nlp,de_nlp,max_length,lower,sos_token,eos_token):\n",
    "    en_tokens=[token.text for token in en_nlp.tokenizer(text[\"en\"])][:max_length]\n",
    "    de_tokens=[token.text for token in de_nlp.tokenizer(text[\"de\"])][:max_length]\n",
    "    if lower :\n",
    "        en_tokens=[token.lower() for token in en_tokens]\n",
    "        de_tokens=[token.lower() for token in de_tokens]\n",
    "    en_tokens=[sos_token]+ en_tokens+[eos_token]\n",
    "    de_tokens=[sos_token]+de_tokens+[eos_token]\n",
    "    return {\"en_tokens\":en_tokens,\"de_tokens\":de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faa327-2e1a-4b62-a8a4-eedd27c51d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=1000\n",
    "lower=True\n",
    "sos_token=\"<sos>\"\n",
    "eos_token=\"<eos>\"\n",
    "fn_kwargs={\n",
    "    \"en_nlp\":en_nlp,\n",
    "    \"de_nlp\":de_nlp,\n",
    "    \"max_length\":max_length,\n",
    "    \"lower\":lower,\n",
    "    \"sos_token\":sos_token,\n",
    "    \"eos_token\":eos_token\n",
    "}\n",
    "\n",
    "train_data=train_data.map(tokenize_text,fn_kwargs=fn_kwargs)\n",
    "valid_data=valid_data.map(tokenize_text,fn_kwargs=fn_kwargs)\n",
    "test_data=test_data.map(tokenize_text,fn_kwargs=fn_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134b3e8-4d68-480b-b22a-ff5ec6790a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca0ae7-15c8-442a-a5a0-874819de211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f87f5-05d4-4a1b-a4aa-9260fcd3e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91e872-1178-44de-a912-a5244779e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq=2\n",
    "unk_token=\"<unk>\"\n",
    "pad_token=\"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be5303-ed09-4ba5-bea7-61000528a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens\n",
    ")\n",
    "de_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78cbba-fba2-420c-8804-cda07155227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_vocab), len(de_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a84992-b8f7-4cb5-9df9-b3fff4e07b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265532f-c64c-49f5-a029-1beecc14825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c387a-d8a8-4400-aac1-3806cf5cc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbf806-76f8-4ca9-9c94-20c296ef410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab[\"The\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfde46a-6f75-4ed3-93d3-c26c85cc2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.get_itos()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4995ee-ce69-41d3-a8d3-485fd27c3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"i\", \"love\", \"watching\", \"crime\", \"shows\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54f1e2-14c9-4a72-a1c4-6c2a59f9865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_num(text,en_vocab,de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(text[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(text[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1953d78-4e90-4ef8-b58c-49a4381a1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs={\"en_vocab\":en_vocab,\"de_vocab\":de_vocab}\n",
    "\n",
    "train_data = train_data.map(token_to_num, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(token_to_num, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(token_to_num, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3e140-8158-4bff-9530-f09f8aa8ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060c933-f4a8-4485-a395-2f34f58b4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e17b7-ee35-4228-a966-a79004098d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807e6c1-b5e6-4aaa-8cdd-1404109c8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"en_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed289705-44e8-49e1-9212-ea259159ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614f1b7-48cc-4050-862e-a332f276ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb12539-9738-482b-88e9-22deaf3d82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.embedding=nn.Embedding(input_dim,embedding_dim)\n",
    "        self.rnn=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=dropout)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,src):\n",
    "        embedded=self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell)=self.rnn(embedded)\n",
    "        return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531ab68-2b3a-419e-b40b-0ce7ea65d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,embedding_dim,hidden_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim=output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.embedding=nn.Embedding(output_dim,embedding_dim)\n",
    "        self.rnn=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=dropout)\n",
    "        self.f_out=nn.Linear(hidden_dim,output_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,input,hidden,cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded=self.dropout(self.embedding(input))\n",
    "        output, (hidden ,cell)= self.rnn(embedded,(hidden,cell))\n",
    "        prediction=self.f_out(output.squeeze(0))\n",
    "        return prediction, hidden ,cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1cf98-6c7e-4558-8670-18895b2c0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.device=device\n",
    "        assert(encoder.hidden_dim==decoder.hidden_dim),\"Hidden dimension of encoder and decoder must be equal\"\n",
    "        assert(encoder.n_layers==decoder.n_layers),\"Encoder and Decoder must have equal number of layers\"\n",
    "\n",
    "    def forward(self,src,trg,teacher_forcing_ratio):\n",
    "        batch_size=trg.shape[1]\n",
    "        trg_length=trg.shape[0]\n",
    "        trg_vocab_size=self.decoder.output_dim\n",
    "        outputs=torch.zeros(trg_length,batch_size,trg_vocab_size).to(self.device)\n",
    "        hidden,cell=self.encoder(src)\n",
    "        input=trg[0,:]\n",
    "        for t in range(1,trg_length):\n",
    "            output,hidden,cell=self.decoder(input,hidden,cell)\n",
    "            outputs[t]=output\n",
    "            teacher_force=random.random()<teacher_forcing_ratio\n",
    "            top=output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad38a50-607c-49ac-bc33-caa1cc0ea578",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=len(de_vocab)\n",
    "output_dim=len(en_vocab)\n",
    "encoder_embedding_dim=256\n",
    "decoder_embedding_dim=256\n",
    "hidden_dim=512\n",
    "n_layers=2\n",
    "encoder_dropout=0.5\n",
    "decoder_dropout=0.5\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder=Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout\n",
    ")\n",
    "\n",
    "decoder=Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout\n",
    ")\n",
    "\n",
    "model=Seq2Seq(encoder,decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a92a1-68fd-4c72-922a-aa48f5c21d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(use):\n",
    "    for name,param in use.named_parameters():\n",
    "        nn.init.uniform_(param.data,-0.08,0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6077706-823b-4182-be65-e2e6f91237a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba4d12-6386-42fe-8524-ea3b38f9f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c04ac4-8bb3-48b2-86d6-3e677c2ccd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data_loader,optimizer,criterion,clip,teacher_forcing_ratio,device):\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        print(f\"Batch {i}\")\n",
    "        src= batch[\"de_ids\"].to(device)\n",
    "        trg= batch[\"en_ids\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(src,trg,teacher_forcing_ratio)\n",
    "        output_dim=output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg=trg[1:].view(-1)\n",
    "        loss=criterion(output,trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca995c-9e2d-4766-896f-89bcfa446d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            print(f\"Batch {i}\")\n",
    "            src = batch[\"de_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84fc7b-5987-428f-98b7-a645a375c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea1bf8-57ba-450a-9c14-b1c8cdbf4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c346906-e993-43f2-b5b2-dccfd477a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b289e-af22-4561-8aa0-7e42df94eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"tut1-model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6fa82-91d3-48eb-8291-8a7f8656db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = de_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74181762-8eac-45a2-aa53-77b45d0db539",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_data[0][\"de\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8667ee-94ed-45ca-b78d-31c1e22c979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4b080-9034-4d02-b1c2-2401326db908",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6fad3c-25d7-4c0d-a535-6354af6d969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc2b33-9e46-4417-a740-1570f7620c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[example[\"en\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b076fa-f4b2-4534-9522-836d0cd7767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"de\"],\n",
    "        model,\n",
    "        en_nlp,\n",
    "        de_nlp,\n",
    "        en_vocab,\n",
    "        de_vocab,\n",
    "        lower,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm.tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e24558-2500-44c0-93fd-0a70071a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bff84-6f96-402d-9ca5-a4a7fe4d1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b3e3a-9bc4-4ca6-be2f-f91a129f99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd5b3d-a52e-4a44-9756-e1510168ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b19c99-7f8a-416f-8e79-9cb94b1327c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5c8c7-bad0-442a-b30d-e3b54ed2507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693817c6-6dd6-4e4b-90ee-597754377188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
